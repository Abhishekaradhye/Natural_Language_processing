{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "lY_D-pz0v_lv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-SN_KWGure1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cU-kFDnu8SA",
        "outputId": "c65b52c0-673a-4dd5-fe2f-7ff31ea32f34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone. Welcome to the competetion. My favourite students are the ones who win!!!\"\n"
      ],
      "metadata": {
        "id": "LlODocwhuvuB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB04MnDFuvrg",
        "outputId": "4a13f5a5-9243-4ed6-acd1-dd608034d9c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to Ethans.',\n",
              " 'My favourite students are the ones who ask questions!!',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "cyCNBEVEwCnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Pass set of words\n",
        "set_of_words = ['run','running','ran','ruining','randomized','afforded']\n",
        "\n",
        "# choose some words to be stemmed\n",
        "for s in set_of_words:\n",
        "  print(s, ':', ps.stem(s))"
      ],
      "metadata": {
        "id": "2sHipQaGuvoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968e5148-8a6b-4dea-ee49-b88e931025bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run : run\n",
            "running : run\n",
            "ran : ran\n",
            "ruining : ruin\n",
            "randomized : random\n",
            "afforded : afford\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "text_1 = \"Hey there, I'm working on stabilizing the drone to have fluent passage above ground level in vision of delivering with quality.\"\n",
        "words = word_tokenize(text_1)\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "metadata": {
        "id": "rJ_CQ7zquvlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd094a8-4a63-4dea-c133-7f0c434ad53e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey  :  hey\n",
            "there  :  there\n",
            ",  :  ,\n",
            "I  :  i\n",
            "'m  :  'm\n",
            "working  :  work\n",
            "on  :  on\n",
            "stabilizing  :  stabil\n",
            "the  :  the\n",
            "drone  :  drone\n",
            "to  :  to\n",
            "have  :  have\n",
            "fluent  :  fluent\n",
            "passage  :  passag\n",
            "above  :  abov\n",
            "ground  :  ground\n",
            "level  :  level\n",
            "in  :  in\n",
            "vision  :  vision\n",
            "of  :  of\n",
            "delivering  :  deliv\n",
            "with  :  with\n",
            "quality  :  qualiti\n",
            ".  :  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "_uOOSymnF5_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"capitals :\", lemmatizer.lemmatize(\"capitals\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "id": "MxFhm0sYuvjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd52ce14-3fcc-4c60-c360-33d8d1e925be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "capitals : capital\n",
            "better : good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StopWords"
      ],
      "metadata": {
        "id": "Wz3ZchqVLFPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentense = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sentense)\n",
        "# Converts the words in word_tokens to lower case and then checks whether\n",
        "# They are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "# With no lower case conversion\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "0THQnzIHuvgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e3a257-01c3-4c74-9ed4-8f0b67d3e299"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyqLNq-l1ucv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF IDF\n"
      ],
      "metadata": {
        "id": "bRZIfPxS1p_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "GwkU1E-tMuoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Multiple documents\n",
        "text = [\"It was the best of times\", \"it was the worst of times\", \"it was the age of wisdom\", \"it was the age of foolishness\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(sorted(vectorizer.vocabulary_))"
      ],
      "metadata": {
        "id": "xJgk_jIuuvdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ffc986-201b-472f-e30c-96ba35c0bc8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'best', 'foolishness', 'it', 'of', 'the', 'times', 'was', 'wisdom', 'worst']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "wXGRVUjKuvVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533e6265-55fb-430d-de0e-1d4596910388"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 10)\n",
            "[[0 1 0 1 1 1 1 1 0 0]\n",
            " [0 0 0 1 1 1 1 1 0 1]\n",
            " [1 0 0 1 1 1 0 1 1 0]\n",
            " [1 0 1 1 1 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode another document\n",
        "text2 = [\"the the the times\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "0y-JHxxJuvSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7352ca6b-ac0f-42cc-d459-e8e0d12ecb90"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 3 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency: This summarizes how often a given word appears within a document.\n",
        "# Inverse Document Frequency: This downscales words that appear a lot across documents."
      ],
      "metadata": {
        "id": "_R215FQvuvQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"It was the best of times\", \"it was the worst of times\", \"it was the age of wisdom\", \"it was the age of foolishness\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(sorted(vectorizer.vocabulary_))\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])"
      ],
      "metadata": {
        "id": "IWuIvj9RuvNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c97c06-a1de-425a-b0c8-51d302556ef0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'best', 'foolishness', 'it', 'of', 'the', 'times', 'was', 'wisdom', 'worst']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "id": "IZjS_B1auvKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75944e10-aca3-4618-e924-f3d734397686"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.51082562 1.91629073 1.91629073 1.         1.         1.\n",
            " 1.51082562 1.         1.91629073 1.91629073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N_GRAMS"
      ],
      "metadata": {
        "id": "Ce05KcSIRc9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "sentence = 'I reside in Bengaluru.'\n",
        "n = 2\n",
        "unigrams = ngrams(sentence.split(), n)\n",
        "for grams in unigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "id": "jp02CuPouu_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea06651-5520-4782-d56e-dc255beb8505"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'reside')\n",
            "('reside', 'in')\n",
            "('in', 'Bengaluru.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hf9YLX3Quu3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}